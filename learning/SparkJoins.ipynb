{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Spark SQL is module in PySpark which is used to process the Structured data. This allows to integrate SQL queries with the spark program  and allows to process in distrbuted and fault tolorent manner.\n",
        "\n",
        "Running SQL-like queries in PySpark constists several steps:\n",
        "\n",
        "* Start by initializing a SparkSession. This is the entry point to PySpark.\n",
        "* Load your data into a DataFrame.\n",
        "* Register the DataFrame as a temporary view so that you can query it using SQL,where we can run our SQL Queries with spark.sql() method\n",
        "* Process the results returned by the SQL query. You can further manipulate the results as needed.\n",
        "\n",
        "# SQL operations illustrated in this notebook\n",
        "Before illustraing the PySpark Sql operations, how join operations in PySpark is perfromed is shown.\n",
        "* select\n",
        "* filter\n",
        "* order by\n",
        "* group by\n",
        "* SQL Joins\n",
        "   - inner join\n",
        "   - left join\n",
        "   - right join\n",
        "   - full outer join\n",
        "   - cross join\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Yqk77m4_Z1nO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YM5A-mqZQhu"
      },
      "outputs": [],
      "source": [
        "# import necessary liberaries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the spark session\n",
        "spark=SparkSession.builder.appName('sparkSQL').getOrCreate()\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "SysdOESOZbeh",
        "outputId": "48a7f92b-07d6-41f0-dd2b-309fb1eeff41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78ed1e0eb0a0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://9591dc5c6095:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.3</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>sparkSQL</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the data from csv file\n",
        "uber_data=spark.read.csv('uber_data.csv',header=True,inferSchema=True)\n",
        "uber_data.show(10)\n",
        "uber_data.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuCJ6G4EZhZr",
        "outputId": "06eec491-04e2-49c6-db5b-f0536d6be191"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "|     Date|Time (Local)|Eyeballs |Zeroes |Completed Trips |Requests |Unique Drivers|\n",
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "|10-Sep-12|           7|        5|      0|               2|        2|             9|\n",
            "|     NULL|           8|        6|      0|               2|        2|            14|\n",
            "|     NULL|           9|        8|      3|               0|        0|            14|\n",
            "|     NULL|          10|        9|      2|               0|        1|            14|\n",
            "|     NULL|          11|       11|      1|               4|        4|            11|\n",
            "|     NULL|          12|       12|      0|               2|        2|            11|\n",
            "|     NULL|          13|        9|      1|               0|        0|             9|\n",
            "|     NULL|          14|       12|      1|               0|        0|             9|\n",
            "|     NULL|          15|       11|      2|               1|        2|             7|\n",
            "|     NULL|          16|       11|      2|               3|        4|             6|\n",
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "only showing top 10 rows\n",
            "\n",
            "root\n",
            " |-- Date: string (nullable = true)\n",
            " |-- Time (Local): integer (nullable = true)\n",
            " |-- Eyeballs : integer (nullable = true)\n",
            " |-- Zeroes : integer (nullable = true)\n",
            " |-- Completed Trips : integer (nullable = true)\n",
            " |-- Requests : integer (nullable = true)\n",
            " |-- Unique Drivers: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create tempurary view for executing SQL queries\n",
        "uber_data.createOrReplaceTempView('rw_uber')"
      ],
      "metadata": {
        "id": "dfEQAfWBbWva"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the tempuraty table is created\n",
        "spark.catalog.listTables()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1FRzVLRxbylY",
        "outputId": "7bafd35a-11bd-43d5-8455-aeb106ac229a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Table(name='rw_uber', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting from spark dataframe\n",
        "uber_data.select('Date','Time (Local)','Eyeballs ').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfsEpHB6cKmL",
        "outputId": "e9abd67c-b02a-47e5-9e0e-735c890ad782"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+---------+\n",
            "|     Date|Time (Local)|Eyeballs |\n",
            "+---------+------------+---------+\n",
            "|10-Sep-12|           7|        5|\n",
            "|     NULL|           8|        6|\n",
            "|     NULL|           9|        8|\n",
            "|     NULL|          10|        9|\n",
            "|     NULL|          11|       11|\n",
            "+---------+------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# performing the same operation with the spark sql\n",
        "spark.sql('select `Date`,`Time (Local)`,`Eyeballs ` from rw_uber').show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cJo2Bd9ckXy",
        "outputId": "966e0680-5dcf-466c-eb1d-5c40556aab7e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+---------+\n",
            "|     Date|Time (Local)|Eyeballs |\n",
            "+---------+------------+---------+\n",
            "|10-Sep-12|           7|        5|\n",
            "|     NULL|           8|        6|\n",
            "|     NULL|           9|        8|\n",
            "|     NULL|          10|        9|\n",
            "|     NULL|          11|       11|\n",
            "+---------+------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering the records\n",
        "spark.sql('select * from rw_uber') \\\n",
        "      .where(\" 'Eyeballs'>='1'\")\\\n",
        "      .show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AHayPVPaczIH",
        "outputId": "6b038835-009f-4b7f-8c55-415fe4352e94"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "|     Date|Time (Local)|Eyeballs |Zeroes |Completed Trips |Requests |Unique Drivers|\n",
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "|10-Sep-12|           7|        5|      0|               2|        2|             9|\n",
            "|     NULL|           8|        6|      0|               2|        2|            14|\n",
            "|     NULL|           9|        8|      3|               0|        0|            14|\n",
            "|     NULL|          10|        9|      2|               0|        1|            14|\n",
            "|     NULL|          11|       11|      1|               4|        4|            11|\n",
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sorting the records\n",
        "spark.sql('select * from rw_uber') \\\n",
        "      .where(\" 'Eyeballs'>='1'\")\\\n",
        "      .orderBy('Date')\\\n",
        "      .show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUp5GWfAdHlV",
        "outputId": "9349d90a-ca05-44e3-948a-7564fe4a636f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+------------+---------+-------+----------------+---------+--------------+\n",
            "|Date|Time (Local)|Eyeballs |Zeroes |Completed Trips |Requests |Unique Drivers|\n",
            "+----+------------+---------+-------+----------------+---------+--------------+\n",
            "|NULL|          13|        9|      1|               0|        0|             9|\n",
            "|NULL|          18|       11|      1|               3|        4|             7|\n",
            "|NULL|          14|       12|      1|               0|        0|             9|\n",
            "|NULL|           8|        6|      0|               2|        2|            14|\n",
            "|NULL|          15|       11|      2|               1|        2|             7|\n",
            "+----+------------+---------+-------+----------------+---------+--------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select * from rw_uber where 'Eyeballs' >='1' order by 'Eyeballs' limit 5 \") \\\n",
        "     .show()\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQDTy05OdsQM",
        "outputId": "0da3f86a-57ea-498e-9887-4499e5a80d29"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "|     Date|Time (Local)|Eyeballs |Zeroes |Completed Trips |Requests |Unique Drivers|\n",
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "|10-Sep-12|           7|        5|      0|               2|        2|             9|\n",
            "|     NULL|           8|        6|      0|               2|        2|            14|\n",
            "|     NULL|           9|        8|      3|               0|        0|            14|\n",
            "|     NULL|          10|        9|      2|               0|        1|            14|\n",
            "|     NULL|          11|       11|      1|               4|        4|            11|\n",
            "+---------+------------+---------+-------+----------------+---------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# group by\n",
        "uber_data.groupBy(\"Eyeballs \").count() \\\n",
        "  .show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1HNi9r6d9S5",
        "outputId": "fe2c4099-ae2f-4d8c-defc-46f57a8b8c4c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----+\n",
            "|Eyeballs |count|\n",
            "+---------+-----+\n",
            "|       31|    3|\n",
            "|       53|    2|\n",
            "|       34|    6|\n",
            "|       28|    5|\n",
            "|       26|    5|\n",
            "|       27|    2|\n",
            "|       44|    1|\n",
            "|       12|    7|\n",
            "|       22|    9|\n",
            "|       47|    1|\n",
            "|        1|   19|\n",
            "|       13|    6|\n",
            "|        6|    8|\n",
            "|       16|   10|\n",
            "|        3|   14|\n",
            "|       20|   12|\n",
            "|       40|    2|\n",
            "|       94|    1|\n",
            "|        5|   11|\n",
            "|       19|   14|\n",
            "+---------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"select `Eyeballs `,count(*) from rw_uber group by `Eyeballs `\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOyOS1z-eeiR",
        "outputId": "a4032ba9-e386-444a-ab5a-561b9d1a90bc"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+--------+\n",
            "|Eyeballs |count(1)|\n",
            "+---------+--------+\n",
            "|       31|       3|\n",
            "|       53|       2|\n",
            "|       34|       6|\n",
            "|       28|       5|\n",
            "|       26|       5|\n",
            "|       27|       2|\n",
            "|       44|       1|\n",
            "|       12|       7|\n",
            "|       22|       9|\n",
            "|       47|       1|\n",
            "|        1|      19|\n",
            "|       13|       6|\n",
            "|        6|       8|\n",
            "|       16|      10|\n",
            "|        3|      14|\n",
            "|       20|      12|\n",
            "|       40|       2|\n",
            "|       94|       1|\n",
            "|        5|      11|\n",
            "|       19|      14|\n",
            "+---------+--------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PySpark joins**  are used to combine two or more that two spark dataframes. I provides all the basic join operations that are avaliable in SQL.\n",
        "These are the wider transformations that involves the data suffeling accross the network.\n",
        "However, **PySpark sql** are more optimized by default.Although PySpark sql has some performance issue, nderstanding how to effectively utilize PySpark joins is essential for conducting comprehensive data analysis, building data pipelines, and deriving valuable insights from large-scale datasets.\n",
        "\n",
        "### pyapark Join syntax:\n",
        "**join(self, other, on=None, how=None)**\n",
        "\n",
        "**join()** function is used to join two dataframes, where **other** is the data frame which you want to join with, **on** is the name of the field with wiich table will be joined, **how**  means type of the join that you want to perfrom.\n",
        "\n",
        "We can also write join expresssion by adding where() and filter() methods on dataframe and can have join operation on the multiple columns."
      ],
      "metadata": {
        "id": "Pq358MEKhKVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating dummy dataframe\n",
        "emp = [(1,\"Smith\",-1,\"2018\",\"10\",\"M\",3000), \\\n",
        "    (2,\"Rose\",1,\"2010\",\"20\",\"M\",4000), \\\n",
        "    (3,\"Williams\",1,\"2010\",\"10\",\"M\",1000), \\\n",
        "    (4,\"Jones\",2,\"2005\",\"10\",\"F\",2000), \\\n",
        "    (5,\"Brown\",2,\"2010\",\"40\",\"\",-1), \\\n",
        "      (6,\"Brown\",2,\"2010\",\"50\",\"\",-1) \\\n",
        "  ]\n",
        "empColumns = [\"emp_id\",\"name\",\"superior_emp_id\",\"year_joined\", \\\n",
        "       \"emp_dept_id\",\"gender\",\"salary\"]\n",
        "\n",
        "empDF = spark.createDataFrame(data=emp, schema = empColumns)\n",
        "empDF.printSchema()\n",
        "empDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C-em7EKem0x",
        "outputId": "fde945e6-d44f-48e3-b45c-612c19c691c8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- emp_id: long (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- superior_emp_id: long (nullable = true)\n",
            " |-- year_joined: string (nullable = true)\n",
            " |-- emp_dept_id: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- salary: long (nullable = true)\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dept = [(\"Finance\",10), \\\n",
        "    (\"Marketing\",20), \\\n",
        "    (\"Sales\",30), \\\n",
        "    (\"IT\",40) \\\n",
        "  ]\n",
        "\n",
        "deptColumns = [\"dept_name\",\"dept_id\"]\n",
        "\n",
        "deptDF = spark.createDataFrame(data=dept, schema = deptColumns)\n",
        "\n",
        "deptDF.printSchema()\n",
        "\n",
        "deptDF.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_DiAUMbjqhx",
        "outputId": "9e5bab65-777f-4165-ae05-70a55ce3ffbc"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- dept_name: string (nullable = true)\n",
            " |-- dept_id: long (nullable = true)\n",
            "\n",
            "+---------+-------+\n",
            "|dept_name|dept_id|\n",
            "+---------+-------+\n",
            "|Finance  |10     |\n",
            "|Marketing|20     |\n",
            "|Sales    |30     |\n",
            "|IT       |40     |\n",
            "+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Inner join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"inner\") \\\n",
        "     .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9jWa8fNkGIR",
        "outputId": "6f847887-e79a-4e37-ab2c-8fcdb81b0361"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark Left Outer join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"left\") \\\n",
        "      .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftouter\") \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wRkS1sDDkUyl",
        "outputId": "947a2723-068f-4fb0-813d-890fe9fd7b61"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PySpark right outer join:\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"right\") \\\n",
        "   .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"rightouter\") \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKJJBjLRkrbE",
        "outputId": "3e0124f5-5f15-4523-ec62-ab27f65f37c3"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark full outer join\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"outer\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"full\") \\\n",
        "    .show(truncate=False)\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"fullouter\") \\\n",
        "    .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNhp6I8ik5yq",
        "outputId": "4b320747-35dc-4bb1-a176-89efab5ac847"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark left semi join\n",
        "'''\n",
        "lest semi join returns only the rows in left table for which there is a match in the right table.\n",
        "'''\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftsemi\") \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek-I9BeOlJLJ",
        "outputId": "5125f309-eb4a-4839-9415-1b1dfcdb0af2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |\n",
            "+------+--------+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark left anti join\n",
        "'''\n",
        "lest semi join returns only the rows in left table for which there is  no  match in the right table.\n",
        "'''\n",
        "empDF.join(deptDF,empDF.emp_dept_id ==  deptDF.dept_id,\"leftanti\") \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFK8txpXlzt8",
        "outputId": "d4281154-b27d-43bf-cbba-9f5a22269192"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|emp_id|name |superior_emp_id|year_joined|emp_dept_id|gender|salary|\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "|6     |Brown|2              |2010       |50         |      |-1    |\n",
            "+------+-----+---------------+-----------+-----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySpark self join\n",
        "'''joining table with itself'''\n",
        "empDF.alias(\"emp1\").join(empDF.alias(\"emp2\"), \\\n",
        "    col(\"emp1.superior_emp_id\") == col(\"emp2.emp_id\"),\"inner\") \\\n",
        "    .select(col(\"emp1.emp_id\"),col(\"emp1.name\"), \\\n",
        "      col(\"emp2.emp_id\").alias(\"superior_emp_id\"), \\\n",
        "      col(\"emp2.name\").alias(\"superior_emp_name\")) \\\n",
        "   .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hhS6w2TmBOy",
        "outputId": "f75289d5-5a1a-4f18-8164-40a560d2d510"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------------+\n",
            "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
            "+------+--------+---------------+-----------------+\n",
            "|2     |Rose    |1              |Smith            |\n",
            "|3     |Williams|1              |Smith            |\n",
            "|4     |Jones   |2              |Rose             |\n",
            "|5     |Brown   |2              |Rose             |\n",
            "|6     |Brown   |2              |Rose             |\n",
            "+------+--------+---------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PySpark Sql Joins\n",
        "All above mentions join operations can alternatively performed by using spark SQl. For this create a temporary view using createOrReplaceTempView(), then use the spark.sql() to execute the join query."
      ],
      "metadata": {
        "id": "U4_-aVtKmwTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating tempurary views\n",
        "empDF.createOrReplaceTempView(\"EMP\")\n",
        "deptDF.createOrReplaceTempView(\"DEPT\")\n",
        "\n",
        "# inner join\n",
        "joinDF2 = spark.sql(\"select * from EMP e INNER JOIN DEPT d ON e.emp_dept_id == d.dept_id\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyi62jWGnGI-",
        "outputId": "fc3895d7-f700-4a14-8387-f572699937d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# left outer join\n",
        "joinDF2 = spark.sql(\"select * from EMP e LEFT JOIN DEPT d ON e.emp_dept_id = d.dept_id\") \\\n",
        "  .show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTRVDfvCoC3d",
        "outputId": "2e85c9b5-4145-4066-c0af-b25e18303cb3"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# full outer join\n",
        "joinDF2 = spark.sql(\"select * from EMP e FULL JOIN DEPT d ON e.emp_dept_id = d.dept_id\") \\\n",
        "  .show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MU5DXO90oMC9",
        "outputId": "40383d8f-5649-4a20-a3dc-48cee7484db5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "|6     |Brown   |2              |2010       |50         |      |-1    |NULL     |NULL   |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# right outer join\n",
        "joinDF2 = spark.sql(\"select * from EMP e RIGHT JOIN DEPT d ON e.emp_dept_id = d.dept_id\") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnMLKYfFovWe",
        "outputId": "59a7c58d-7f31-47d1-c9dc-b652d2de3fd8"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|NULL  |NULL    |NULL           |NULL       |NULL       |NULL  |NULL  |Sales    |30     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# left semi join\n",
        "joinDF2 = spark.sql(\"select * from EMP e LEFT JOIN DEPT d ON e.emp_dept_id = d.dept_id where e.emp_dept_id==d.dept_id \") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Z9hBXxenprC",
        "outputId": "66eff416-085a-408b-acb7-dc4b264c0389"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name    |superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|1     |Smith   |-1             |2018       |10         |M     |3000  |Finance  |10     |\n",
            "|3     |Williams|1              |2010       |10         |M     |1000  |Finance  |10     |\n",
            "|4     |Jones   |2              |2005       |10         |F     |2000  |Finance  |10     |\n",
            "|2     |Rose    |1              |2010       |20         |M     |4000  |Marketing|20     |\n",
            "|5     |Brown   |2              |2010       |40         |      |-1    |IT       |40     |\n",
            "+------+--------+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# anti left join\n",
        "joinDF2 = spark.sql(\"select * from EMP e LEFT JOIN DEPT d ON e.emp_dept_id = d.dept_id where e.emp_dept_id != d.dept_id \") \\\n",
        "  .show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEIX8l-IpjVh",
        "outputId": "9a0e9e67-d607-443d-d579-e6f0ff37ec61"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+----+---------------+-----------+-----------+------+------+---------+-------+\n",
            "|emp_id|name|superior_emp_id|year_joined|emp_dept_id|gender|salary|dept_name|dept_id|\n",
            "+------+----+---------------+-----------+-----------+------+------+---------+-------+\n",
            "+------+----+---------------+-----------+-----------+------+------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# self join\n",
        "spark.sql(\"\"\"\n",
        "  SELECT emp1.emp_id, emp1.name,\n",
        "         emp2.emp_id AS superior_emp_id,\n",
        "         emp2.name AS superior_emp_name\n",
        "  FROM EMP emp1\n",
        "  INNER JOIN EMP emp2\n",
        "    ON emp1.superior_emp_id = emp2.emp_id\n",
        "\"\"\").show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDGJpiKApJms",
        "outputId": "c4bfba5a-0df4-4a4c-e7fd-f65982ffa902"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------+---------------+-----------------+\n",
            "|emp_id|name    |superior_emp_id|superior_emp_name|\n",
            "+------+--------+---------------+-----------------+\n",
            "|2     |Rose    |1              |Smith            |\n",
            "|3     |Williams|1              |Smith            |\n",
            "|4     |Jones   |2              |Rose             |\n",
            "|5     |Brown   |2              |Rose             |\n",
            "|6     |Brown   |2              |Rose             |\n",
            "+------+--------+---------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0fpY1hUVq7UA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}